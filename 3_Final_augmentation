{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4","authorship_tag":"ABX9TyPAEdhe8rRvLBzJtsXjrHL9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["\n","\n","This notebook is dedicated to the final stage of data augmentation, completing the dataset before moving on to preprocessing and cleaning, which are handled in a separate notebook. It ensures that all augmentation techniques are applied systematically to enrich the dataset for downstream tasks. Notes on methods are incuded in the script to maintain clarity and reproducibility."],"metadata":{"id":"CJFhRxcnAeIy"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WMPDAAUhW4Zj","executionInfo":{"status":"ok","timestamp":1732140423854,"user_tz":0,"elapsed":26494,"user":{"displayName":"Sarah Braid","userId":"04839811919803455107"}},"outputId":"b7372f4d-7823-4c79-8563-0c01371dc88d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PUAKoVxp608V"},"outputs":[],"source":["import pandas as pd\n","\n","# Load the two original files from Google Drive\n","file1_path = \"/content/drive/MyDrive/SMART_Goals_v2.csv\"\n","file2_path = \"/content/drive/MyDrive/SMART_Goals_extra_v3.csv\"\n","\n","# Read the CSV files\n","df1 = pd.read_csv(file1_path)\n","df2 = pd.read_csv(file2_path)\n","\n","# Combine them\n","combined_df = pd.concat([df1, df2], ignore_index=True)\n","\n","# Add the \"Technique\" column and label it as \"Original\"\n","combined_df['Technique'] = 'Original'\n","\n","# Save the combined file\n","combined_file_path = \"/content/drive/MyDrive/combined_original_goals.csv\"\n","combined_df.to_csv(combined_file_path, index=False)\n"]},{"cell_type":"code","source":["!pip install googletrans==4.0.0-rc1 nltk pandas"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9sCY5U9vcjoW","executionInfo":{"status":"ok","timestamp":1732121416284,"user_tz":0,"elapsed":6444,"user":{"displayName":"Sarah Braid","userId":"04839811919803455107"}},"outputId":"c229df17-0142-448a-aa17-6d8540005059"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting googletrans==4.0.0-rc1\n","  Downloading googletrans-4.0.0rc1.tar.gz (20 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n","Collecting httpx==0.13.3 (from googletrans==4.0.0-rc1)\n","  Downloading httpx-0.13.3-py3-none-any.whl.metadata (25 kB)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2024.8.30)\n","Collecting hstspreload (from httpx==0.13.3->googletrans==4.0.0-rc1)\n","  Downloading hstspreload-2024.11.1-py3-none-any.whl.metadata (2.1 kB)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.1)\n","Collecting chardet==3.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n","  Downloading chardet-3.0.4-py2.py3-none-any.whl.metadata (3.2 kB)\n","Collecting idna==2.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n","  Downloading idna-2.10-py2.py3-none-any.whl.metadata (9.1 kB)\n","Collecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans==4.0.0-rc1)\n","  Downloading rfc3986-1.5.0-py2.py3-none-any.whl.metadata (6.5 kB)\n","Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n","  Downloading httpcore-0.9.1-py3-none-any.whl.metadata (4.6 kB)\n","Collecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n","  Downloading h11-0.9.0-py2.py3-none-any.whl.metadata (8.1 kB)\n","Collecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n","  Downloading h2-3.2.0-py2.py3-none-any.whl.metadata (32 kB)\n","Collecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n","  Downloading hyperframe-5.2.0-py2.py3-none-any.whl.metadata (7.2 kB)\n","Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n","  Downloading hpack-3.0.0-py2.py3-none-any.whl.metadata (7.0 kB)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n","Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n","Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading idna-2.10-py2.py3-none-any.whl (58 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n","Downloading hstspreload-2024.11.1-py3-none-any.whl (1.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n","Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n","Building wheels for collected packages: googletrans\n","  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for googletrans: filename=googletrans-4.0.0rc1-py3-none-any.whl size=17397 sha256=2db36e3751056bb5e350d7847ae11004f63b82e8e5d83837079be18a5b22cc22\n","  Stored in directory: /root/.cache/pip/wheels/c0/59/9f/7372f0cf70160fe61b528532e1a7c8498c4becd6bcffb022de\n","Successfully built googletrans\n","Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\n","  Attempting uninstall: h11\n","    Found existing installation: h11 0.14.0\n","    Uninstalling h11-0.14.0:\n","      Successfully uninstalled h11-0.14.0\n","  Attempting uninstall: chardet\n","    Found existing installation: chardet 5.2.0\n","    Uninstalling chardet-5.2.0:\n","      Successfully uninstalled chardet-5.2.0\n","  Attempting uninstall: idna\n","    Found existing installation: idna 3.10\n","    Uninstalling idna-3.10:\n","      Successfully uninstalled idna-3.10\n","  Attempting uninstall: httpcore\n","    Found existing installation: httpcore 1.0.6\n","    Uninstalling httpcore-1.0.6:\n","      Successfully uninstalled httpcore-1.0.6\n","  Attempting uninstall: httpx\n","    Found existing installation: httpx 0.27.2\n","    Uninstalling httpx-0.27.2:\n","      Successfully uninstalled httpx-0.27.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","langsmith 0.1.143 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\n","openai 1.54.4 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed chardet-3.0.4 googletrans-4.0.0rc1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2024.11.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\n"]}]},{"cell_type":"markdown","source":["\n","Back-translation was tested using both German and French as intermediate languages. German translations were observed to produce outputs with lower semantic similarity to the original text compared to French, making it less effective for maintaining consistency. Microsoft Translator on Azure was used for this process, as it allowed systematic and scalable translations suitable for programmatic use. Google Translate's free version was initially considered but found unsuitable due to limitations in supporting large-scale automated workflows."],"metadata":{"id":"vS6Ga7-FP-mc"}},{"cell_type":"code","source":["import pandas as pd\n","import requests\n","import time\n","from google.colab import drive\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# Microsoft Translator API details\n","AZURE_TRANSLATOR_KEY = '6kaDaVeiUxTQpnz8BMSl9CD0YouJLTpv5yDnb7N28N5pfUrtPHTJJQQJ99AJACmepeSXJ3w3AAAbACOGxpMg'  # Replace with your actual key\n","AZURE_TRANSLATOR_ENDPOINT = 'https://api.cognitive.microsofttranslator.com/'\n","AZURE_REGION = 'uksouth'  # Replace with the Azure region of your resource, e.g., 'westeurope'\n","\n","# Function to translate text using Microsoft Translator\n","def translate_text(text, from_lang, to_lang):\n","    headers = {\n","        'Ocp-Apim-Subscription-Key': AZURE_TRANSLATOR_KEY,\n","        'Content-type': 'application/json',\n","        'Ocp-Apim-Subscription-Region': AZURE_REGION\n","    }\n","    params = {\n","        'api-version': '3.0',\n","        'from': from_lang,\n","        'to': [to_lang]\n","    }\n","    body = [{'text': text}]\n","    response = requests.post(f\"{AZURE_TRANSLATOR_ENDPOINT}/translate\", params=params, headers=headers, json=body)\n","    response.raise_for_status()  # Raises an error for unsuccessful requests\n","    return response.json()[0]['translations'][0]['text']\n","\n","# Function to perform back translation\n","def back_translate(text, retries=3):\n","    attempt = 0\n","    while attempt < retries:\n","        try:\n","            # Step 1: Translate to German\n","            german_translation = translate_text(text, 'en', 'de')\n","            time.sleep(0.5)  # Small delay to avoid rate limiting\n","\n","            # Step 2: Back-translate to English\n","            english_translation = translate_text(german_translation, 'de', 'en')\n","            time.sleep(0.5)  # Small delay to avoid rate limiting\n","            return english_translation, \"Back Translation (German)\"\n","\n","        except requests.exceptions.RequestException as e:\n","            attempt += 1\n","            print(f\"Attempt {attempt} failed due to error: {e}\")\n","            time.sleep(2)  # Delay before retrying\n","\n","    print(f\"Translation failed for text: '{text[:50]}...' after {retries} attempts.\")\n","    return text, \"Translation Failed\"\n","\n","# Load the original CSV file from Google Drive\n","original_df = pd.read_csv('/content/drive/My Drive/combined_original_goals.csv')\n","\n","# Sample 1000 rows and split into smaller chunks\n","backtranslated_data = original_df.sample(1000, random_state=42).copy()\n","chunks = [backtranslated_data.iloc[i:i+200].copy() for i in range(0, len(backtranslated_data), 200)]\n","\n","# Process each chunk and save incrementally with progress indicators\n","augmented_rows = []\n","total_rows_processed = 0\n","for i, chunk in enumerate(chunks, 1):\n","    print(f\"\\nProcessing chunk {i} of {len(chunks)}...\")\n","\n","    # Apply Back Translation only to the \"Vague Goal\" column\n","    translations = [back_translate(text) for text in chunk['Vague Goal']]\n","    chunk['Augmented Vague Goal'], chunk['Technique'] = zip(*translations)\n","\n","    # Count successful and failed translations\n","    success_count = sum(1 for t in chunk['Technique'] if t == \"Back Translation (German)\")\n","    fail_count = sum(1 for t in chunk['Technique'] if t == \"Translation Failed\")\n","    print(f\"Chunk {i} summary: {success_count} successful, {fail_count} failed translations.\")\n","\n","    # Append the chunk results to the list\n","    augmented_rows.append(chunk)\n","\n","    # Update the total rows processed and show progress within each chunk\n","    total_rows_processed += len(chunk)\n","    print(f\"Completed {total_rows_processed} out of 1000 total rows.\")\n","\n","    # Save incremental progress to a temporary file\n","    temp_df = pd.concat(augmented_rows, ignore_index=True)\n","    temp_df.to_csv('/content/drive/My Drive/combined_backtranslation_temp.csv', index=False)\n","    print(f\"Saved progress for chunk {i} to '/content/drive/My Drive/combined_backtranslation_temp.csv'.\")\n","\n","# Combine all chunks and save the final file\n","backtranslated_final = pd.concat(augmented_rows, ignore_index=True)\n","backtranslated_final.to_csv('/content/drive/My Drive/combined_backtranslation_goals.csv', index=False)\n","\n","print(\"Backtranslation completed and saved as 'combined_backtranslation_goals.csv'.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U4kttIk_x1KP","executionInfo":{"status":"ok","timestamp":1730148351407,"user_tz":0,"elapsed":1581319,"user":{"displayName":"Sarah Braid","userId":"04839811919803455107"}},"outputId":"047dbaf6-29e0-450b-83e9-9a6d82ac5742"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","\n","Processing chunk 1 of 5...\n","Chunk 1 summary: 200 successful, 0 failed translations.\n","Completed 200 out of 1000 total rows.\n","Saved progress for chunk 1 to '/content/drive/My Drive/combined_backtranslation_temp.csv'.\n","\n","Processing chunk 2 of 5...\n","Chunk 2 summary: 200 successful, 0 failed translations.\n","Completed 400 out of 1000 total rows.\n","Saved progress for chunk 2 to '/content/drive/My Drive/combined_backtranslation_temp.csv'.\n","\n","Processing chunk 3 of 5...\n","Chunk 3 summary: 200 successful, 0 failed translations.\n","Completed 600 out of 1000 total rows.\n","Saved progress for chunk 3 to '/content/drive/My Drive/combined_backtranslation_temp.csv'.\n","\n","Processing chunk 4 of 5...\n","Chunk 4 summary: 200 successful, 0 failed translations.\n","Completed 800 out of 1000 total rows.\n","Saved progress for chunk 4 to '/content/drive/My Drive/combined_backtranslation_temp.csv'.\n","\n","Processing chunk 5 of 5...\n","Attempt 1 failed due to error: HTTPSConnectionPool(host='api.cognitive.microsofttranslator.com', port=443): Max retries exceeded with url: /translate?api-version=3.0&from=en&to=de (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7c704eb9f4f0>: Failed to establish a new connection: [Errno 111] Connection refused'))\n","Attempt 2 failed due to error: HTTPSConnectionPool(host='api.cognitive.microsofttranslator.com', port=443): Max retries exceeded with url: /translate?api-version=3.0&from=de&to=en (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7c704eb9ce50>: Failed to establish a new connection: [Errno 111] Connection refused'))\n","Chunk 5 summary: 200 successful, 0 failed translations.\n","Completed 1000 out of 1000 total rows.\n","Saved progress for chunk 5 to '/content/drive/My Drive/combined_backtranslation_temp.csv'.\n","Backtranslation completed and saved as 'combined_backtranslation_goals.csv'.\n"]}]},{"cell_type":"markdown","source":["Simplicity-based augmentation was employed to break down complex GPT patterns and introduce greater variety in the dataset. This approach aimed to ensure that the dataset contained diverse yet coherent examples, enhancing model robustness."],"metadata":{"id":"2hXrAlvnRruF"}},{"cell_type":"code","source":["import pandas as pd\n","import requests\n","import time\n","from google.colab import drive\n","import re\n","\n","\n","#simplification augmentataion\n","\n","# Load the original goals file\n","original_file_path = '/content/drive/My Drive/combined_original_goals.csv'\n","df = pd.read_csv(original_file_path)\n","\n","# Simplification function\n","def simplify_text(text):\n","    # Step 1: Remove common filler words\n","    filler_words = [\"just\", \"really\", \"very\", \"actually\", \"a bit\", \"some\", \"kind of\", \"sort of\", \"maybe\", \"perhaps\"]\n","    for word in filler_words:\n","        text = re.sub(r'\\b' + word + r'\\b', '', text)\n","\n","    # Step 2: Cut down to about 20% of words, keeping full sentences\n","    words = text.split()\n","    target_length = int(len(words) * 0.2)\n","\n","    # Find nearest sentence end (., !, ?)\n","    for i in range(target_length, len(words)):\n","        if words[i][-1] in ['.', '!', '?']:\n","            return ' '.join(words[:i+1])\n","\n","    # Default case if no sentence end found within the target length\n","    return ' '.join(words[:target_length])\n","\n","\n","\n","# Apply simplification to the 'Vague Goal' column\n","df['Simplified Vague Goal'] = df['Vague Goal'].apply(simplify_text)\n","\n","# Rename \"Simplified Vague Goal\" to \"Augmented Vague Goal\" in the same dataframe\n","df.rename(columns={'Simplified Vague Goal': 'Augmented Vague Goal'}, inplace=True)\n","\n","# Save the entire dataframe with all columns, including \"Augmented Vague Goal\"\n","simplified_file_path = '/content/drive/My Drive/combined_simplification_goals.csv'\n","df.to_csv(simplified_file_path, index=False)\n","\n","print(\"Simplification completed and saved with all columns intact.\")\n","print(df.columns)\n","\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rCbBXyw26F3_","executionInfo":{"status":"ok","timestamp":1732122734823,"user_tz":0,"elapsed":1004,"user":{"displayName":"Sarah Braid","userId":"04839811919803455107"}},"outputId":"2f6288c0-ff8d-4c0d-cf14-0f243d9001cf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Simplification completed and saved with all columns intact.\n","Index(['Area', 'Vague Goal', 'SMART Goal', 'Technique',\n","       'Augmented Vague Goal'],\n","      dtype='object')\n"]}]},{"cell_type":"markdown","source":["Combining Files to Create a Unified Dataset of Vague Goals and SMART Goals involved merging original vague goals, their augmented variations, and paired SMART goals into a single, cohesive dataset. This step enhanced the dataset’s diversity and robustness, ensuring it was well-structured and suitable for training, testing, and evaluation. By integrating original and augmented data, the dataset reflects real-world input patterns while supporting systematic experimentation and model development.\n","\n","\n","\n"],"metadata":{"id":"bo21ETmMPACT"}},{"cell_type":"code","source":["import pandas as pd\n","\n","# Define paths for each of your individual CSV files\n","original_file = '/content/drive/My Drive/combined_original_goals.csv'\n","back_translation_file = '/content/drive/My Drive/combined_backtranslation_goals_v2.csv'\n","simplification_file = '/content/drive/My Drive/combined_simplification_goals_v2.csv'\n","\n","# Define paths for each of your individual CSV files\n","original_file = '/content/drive/My Drive/combined_original_goals.csv'\n","back_translation_file = '/content/drive/My Drive/combined_backtranslation_goals_v2.csv'\n","simplification_file = '/content/drive/My Drive/combined_simplification_goals_v2.csv'\n","\n","# Load each file into a DataFrame\n","df_original = pd.read_csv(original_file)\n","df_back_translation = pd.read_csv(back_translation_file)\n","df_simplification = pd.read_csv(simplification_file)\n","\n","# Add the 'Technique' column for labeling\n","df_original['Technique'] = 'Original'\n","df_back_translation['Technique'] = 'Back Translation'\n","df_simplification['Technique'] = 'Simplification'\n","\n","# Handle missing columns and align column names\n","# For df_original, add 'Augmented Vague Goal' if it doesn't exist\n","if 'Augmented Vague Goal' not in df_original.columns:\n","    df_original['Augmented Vague Goal'] = df_original['Vague Goal']  # Default to Vague Goal\n","\n","# For df_simplification, rename 'Simplified Vague Goal' to 'Augmented Vague Goal'\n","if 'Simplified Vague Goal' in df_simplification.columns:\n","    df_simplification.rename(columns={'Simplified Vague Goal': 'Augmented Vague Goal'}, inplace=True)\n","\n","# Ensure all DataFrames have the same columns: Area, Vague Goal, Augmented Vague Goal, SMART Goal, Technique\n","columns = ['Area', 'Vague Goal', 'Augmented Vague Goal', 'SMART Goal', 'Technique']\n","df_original = df_original.reindex(columns=columns)\n","df_back_translation = df_back_translation.reindex(columns=columns)\n","df_simplification = df_simplification.reindex(columns=columns)\n","\n","# Combine all DataFrames into a single DataFrame\n","combined_df = pd.concat([df_original, df_back_translation, df_simplification], ignore_index=True)\n","\n","# Save the combined DataFrame to a new CSV file\n","combined_file_path = '/content/drive/My Drive/combined_all_goals_dataset_v3.csv'\n","combined_df.to_csv(combined_file_path, index=False)\n","\n","print(f\"All files combined and saved as '{combined_file_path}'.\")\n","print(combined_df.columns)\n","print(combined_df.head())\n","\n","\n","# Save the combined DataFrame to a new CSV file\n","combined_file_path = '/content/drive/My Drive/combined_all_goals_dataset_v3.csv'\n","combined_df.to_csv(combined_file_path, index=False)\n","\n","print(f\"All files combined and saved as '{combined_file_path}'.\")\n","print(combined_df.columns)\n","print(combined_df.head())\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aMy1Weu_9dQC","executionInfo":{"status":"ok","timestamp":1732140445889,"user_tz":0,"elapsed":3414,"user":{"displayName":"Sarah Braid","userId":"04839811919803455107"}},"outputId":"5801ed8e-6deb-4748-bb9d-2ac0e113e2c4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["All files combined and saved as '/content/drive/My Drive/combined_all_goals_dataset_v3.csv'.\n","Index(['Area', 'Vague Goal', 'Augmented Vague Goal', 'SMART Goal',\n","       'Technique'],\n","      dtype='object')\n","         Area                                         Vague Goal  \\\n","0  leadership  I’d like to explore ways to connect better wit...   \n","1  leadership  \"I'm looking to explore ways to connect better...   \n","2  leadership  I'm looking to explore ways to be more support...   \n","3  leadership  \"I’m hoping to explore ways to enhance my appr...   \n","4  leadership  \"I’m thinking about exploring ways to strength...   \n","\n","                                Augmented Vague Goal  \\\n","0  I’d like to explore ways to connect better wit...   \n","1  \"I'm looking to explore ways to connect better...   \n","2  I'm looking to explore ways to be more support...   \n","3  \"I’m hoping to explore ways to enhance my appr...   \n","4  \"I’m thinking about exploring ways to strength...   \n","\n","                                          SMART Goal Technique  \n","0  By the end of the next quarter, I will enhance...  Original  \n","1  By the end of the next quarter, I will enhance...  Original  \n","2  By the end of the next quarter, I will impleme...  Original  \n","3  Over the next three months, I will enhance my ...  Original  \n","4  Over the next three months, I will enhance my ...  Original  \n","All files combined and saved as '/content/drive/My Drive/combined_all_goals_dataset_v3.csv'.\n","Index(['Area', 'Vague Goal', 'Augmented Vague Goal', 'SMART Goal',\n","       'Technique'],\n","      dtype='object')\n","         Area                                         Vague Goal  \\\n","0  leadership  I’d like to explore ways to connect better wit...   \n","1  leadership  \"I'm looking to explore ways to connect better...   \n","2  leadership  I'm looking to explore ways to be more support...   \n","3  leadership  \"I’m hoping to explore ways to enhance my appr...   \n","4  leadership  \"I’m thinking about exploring ways to strength...   \n","\n","                                Augmented Vague Goal  \\\n","0  I’d like to explore ways to connect better wit...   \n","1  \"I'm looking to explore ways to connect better...   \n","2  I'm looking to explore ways to be more support...   \n","3  \"I’m hoping to explore ways to enhance my appr...   \n","4  \"I’m thinking about exploring ways to strength...   \n","\n","                                          SMART Goal Technique  \n","0  By the end of the next quarter, I will enhance...  Original  \n","1  By the end of the next quarter, I will enhance...  Original  \n","2  By the end of the next quarter, I will impleme...  Original  \n","3  Over the next three months, I will enhance my ...  Original  \n","4  Over the next three months, I will enhance my ...  Original  \n"]}]}]}