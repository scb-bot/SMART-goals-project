{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4","authorship_tag":"ABX9TyPhvL0L7OgNaPYTYFt25UX3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d4qr6swntENZ","executionInfo":{"status":"ok","timestamp":1732528823911,"user_tz":0,"elapsed":22466,"user":{"displayName":"Sarah Braid","userId":"04839811919803455107"}},"outputId":"5061ba18-cb4c-4e62-b2de-5ac569602f48"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["\n","This script processes and cleans the datasets used for SMART Goals generation. It applies consistent preprocessing steps, such as stripping whitespace, converting text to lowercase, and removing quotes, to ensure uniformity across all datasets (train, validation, and test). Key columns, including 'Augmented Vague Goal' and 'SMART Goal,' are checked for missing values, which are removed to maintain data integrity. The cleaned datasets are then saved to new files for subsequent use in model training and evaluation. This preprocessing step is critical for ensuring high-quality inputs throughout the SMART Goals generation pipeline"],"metadata":{"id":"tA07tM0tyBfE"}},{"cell_type":"code","source":["import pandas as pd\n","import warnings\n","import wandb\n","\n","# Suppress warnings\n","warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"transformers\")\n","\n","# Login to Weights & Biases\n","wandb.login()\n","\n","# File paths\n","file_paths = {\n","    \"train\": \"/content/drive/My Drive/train_data.csv\",\n","    \"validation\": \"/content/drive/My Drive/validation_data.csv\",\n","    \"test\": \"/content/drive/My Drive/test_data.csv\"\n","}\n","\n","# Output file names\n","output_files = {\n","    \"train\": \"/content/drive/My Drive/train_data_cleaned.csv\",\n","    \"validation\": \"/content/drive/My Drive/validation_data_cleaned.csv\",\n","    \"test\": \"/content/drive/My Drive/test_data_cleaned.csv\"\n","}\n","\n","# Cleaning function for the entire dataset\n","def clean_entire_dataset(dataset):\n","    # Convert all columns to strings\n","    dataset = dataset.astype(str)\n","\n","    # Clean all columns\n","    for col in dataset.columns:\n","        # Strip whitespace and lowercase\n","        dataset[col] = dataset[col].str.strip().str.lower()\n","\n","        # Remove quotes\n","        dataset[col] = dataset[col].str.replace(r'[\"\\']', '', regex=True)\n","\n","    # Drop rows with NaN values in the two key columns (converted to string \"nan\")\n","    dataset.replace(\"nan\", pd.NA, inplace=True)\n","    dataset.dropna(subset=['Augmented Vague Goal', 'SMART Goal'], inplace=True)\n","\n","    return dataset\n","\n","# Clean and save each dataset\n","for name, path in file_paths.items():\n","    print(f\"Processing {name.capitalize()} data...\")\n","\n","    # Load dataset\n","    data = pd.read_csv(path)\n","\n","    # Clean dataset (all columns treated as strings)\n","    cleaned_data = clean_entire_dataset(data)\n","\n","    # Save cleaned dataset to a new file\n","    save_path = output_files[name]\n","    cleaned_data.to_csv(save_path, index=False)\n","    print(f\"{name.capitalize()} data cleaned and saved to {save_path}\")\n","\n","print(\"All datasets cleaned and saved successfully!\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sp-cjK9EwQto","executionInfo":{"status":"ok","timestamp":1732528834517,"user_tz":0,"elapsed":5052,"user":{"displayName":"Sarah Braid","userId":"04839811919803455107"}},"outputId":"1c060671-4c36-4630-c693-38c459733d90"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbraid781\u001b[0m (\u001b[33mbraid781-northeastern-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"output_type":"stream","name":"stdout","text":["Processing Train data...\n","Train data cleaned and saved to /content/drive/My Drive/train_data_cleaned.csv\n","Processing Validation data...\n","Validation data cleaned and saved to /content/drive/My Drive/validation_data_cleaned.csv\n","Processing Test data...\n","Test data cleaned and saved to /content/drive/My Drive/test_data_cleaned.csv\n","All datasets cleaned and saved successfully!\n"]}]}]}